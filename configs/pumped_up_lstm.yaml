model: 'LSTM'

embed_size: 75

hidden_size: 512

num_layers: 3

epochs: 15

patience: 5

seq_len: 128

learning_rate: 0.0015

log_interval: 5

batch_size: 128

save_path: "models"